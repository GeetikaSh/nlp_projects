{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Request Moby Dick\nThe first step will be to request the Moby Dick HTML file using requests and encoding it to utf-8. Here is the URL to scrape from: https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm","metadata":{},"cell_type":"markdown","id":"253e2c1b-dd14-4792-a964-3321eeadfb42"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Getting the Moby Dick HTML \nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n\n# Setting the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n\n# Extracting the HTML from the request object\nhtml = r.text\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":107,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Getting the Moby Dick HTML \nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n\n# Setting the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n\n# Extracting the HTML from the request object\nhtml = r.text\n","executionCancelledAt":null,"lastExecutedAt":1734478462216,"lastExecutedByKernel":"b0751488-8b51-4213-880d-de196f326519","lastScheduledRunId":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":89,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}]},{"source":"# Get the text from the HTML\n`Beautiful Soup` is a Python library used for web scraping purposes to extract data from HTML and XML documents. It creates parse trees from page source code that can be used to extract data from web pages.","metadata":{},"cell_type":"markdown","id":"84fa352f-9828-4952-b3f1-c9852e49b2e5"},{"source":"# Creating a BeautifulSoup object from the HTML\nhtml_soup = BeautifulSoup(html, 'html.parser')\n\n# Getting the text out of the soup\nmoby_text = html_soup.get_text()","metadata":{"executionCancelledAt":null,"executionTime":238,"lastExecutedAt":1734478462454,"lastExecutedByKernel":"b0751488-8b51-4213-880d-de196f326519","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Creating a BeautifulSoup object from the HTML\nhtml_soup = BeautifulSoup(html, 'html.parser')\n\n# Getting the text out of the soup\nmoby_text = html_soup.get_text()"},"cell_type":"code","id":"26281bee-3637-4754-86b5-df5c47719d2b","outputs":[],"execution_count":90},{"source":"# Extract the words\nA `tokenizer` is a tool used in natural language processing (NLP) that splits text into smaller units, called tokens. These tokens can be words, subwords, or characters, depending on the type of tokenizer used. Tokenization is an essential first step in most NLP tasks like text classification, machine translation, and sentiment analysis","metadata":{},"cell_type":"markdown","id":"ddc5b681-c549-4bd4-be5e-8825a2af19a1"},{"source":"# Creating a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenizing the text\ntokens = tokenizer.tokenize(moby_text)\n\n# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]","metadata":{"executionCancelledAt":null,"executionTime":113,"lastExecutedAt":1734478462568,"lastExecutedByKernel":"b0751488-8b51-4213-880d-de196f326519","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Creating a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenizing the text\ntokens = tokenizer.tokenize(moby_text)\n\n# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]"},"cell_type":"code","id":"0e720845-6ff9-423d-83bd-ad583dac4e7b","outputs":[],"execution_count":91},{"source":"#  Remove stop words in Moby Dick\n`Stop words` are common words (such as \"the\", \"is\", \"in\", \"at\", \"on\", etc.) that are often filtered out during text preprocessing because they donâ€™t carry much meaningful information in many natural language processing (NLP) tasks. These words tend to be frequent in texts but don't contribute to the context or meaning in tasks like text classification, sentiment analysis, or topic modeling.","metadata":{},"cell_type":"markdown","id":"b949a203-4058-413c-89df-0b0332a85b71"},{"source":"# Getting the English stop words from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n\n# A new list to hold Moby Dick with No Stop words\nwords_no_stop = []\n\n# Create a list words_no_stop containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n","metadata":{"executionCancelledAt":null,"executionTime":378,"lastExecutedAt":1734478462946,"lastExecutedByKernel":"b0751488-8b51-4213-880d-de196f326519","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Getting the English stop words from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n\n# A new list to hold Moby Dick with No Stop words\nwords_no_stop = []\n\n# Create a list words_no_stop containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n"},"cell_type":"code","id":"94f0b93e-f0c3-4d2d-9b73-5eb473359f1d","outputs":[],"execution_count":92},{"source":"# Top ten most common words","metadata":{},"cell_type":"markdown","id":"39668a53-e3c8-4318-83cf-36f13eb62be8"},{"source":"# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n\n# Print the top ten words and their counts\nprint(top_ten)","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1734478463011,"lastExecutedByKernel":"b0751488-8b51-4213-880d-de196f326519","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n\n# Print the top ten words and their counts\nprint(top_ten)","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"968d7527-faf2-4ae1-a3cd-cafa6ee590a6","outputs":[{"output_type":"stream","name":"stdout","text":"[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}],"execution_count":93}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}